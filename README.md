# LibraVision: Reconhecimento de Libras em Tempo Real

## ğŸ“‹ VisÃ£o Geral

**LibraVision** Ã© um projeto de VisÃ£o Computacional e InteligÃªncia Artificial que utiliza a cÃ¢mera do computador para reconhecer gestos do alfabeto da LÃ­ngua Brasileira de Sinais (Libras) e os traduz para texto em tempo real na tela.

O sistema captura imagens da webcam, detecta a mÃ£o usando MediaPipe, extrai 21 pontos de referÃªncia em 3D, normaliza os dados e classifica o gesto usando um modelo de Machine Learning (Random Forest), exibindo a letra correspondente com confianÃ§a e suavizaÃ§Ã£o para evitar oscilaÃ§Ãµes.

---

## ğŸ”¬ Como Funciona o Projeto

### Arquitetura e Pipeline

O LibraVision funciona em 4 etapas principais:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   1. COLETA     â”‚ --> â”‚  2. TREINAMENTO â”‚ --> â”‚    3. TESTE     â”‚ --> â”‚  4. TEMPO REAL  â”‚
â”‚   DE DADOS      â”‚     â”‚    DO MODELO    â”‚     â”‚   (OPCIONAL)    â”‚     â”‚   (PRODUÃ‡ÃƒO)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 1ï¸âƒ£ Coleta de Dados (`1_collect_data.py`)

**Objetivo:** Capturar exemplos de gestos de Libras para treinar o modelo.

**Como funciona:**
1. Abre a webcam usando **OpenCV**
2. Usa **MediaPipe Hands** para detectar a mÃ£o na imagem
3. Extrai **21 pontos de referÃªncia (landmarks)** da mÃ£o em coordenadas 3D (x, y, z):
   - Ponto 0: Pulso
   - Pontos 1-4: Polegar
   - Pontos 5-8: Indicador
   - Pontos 9-12: Dedo mÃ©dio
   - Pontos 13-16: Anelar
   - Pontos 17-20: Mindinho

4. Para cada letra do alfabeto, captura mÃºltiplas amostras do gesto
5. Salva os dados em `data/libras_data.csv` com 64 colunas:
   - `label`: letra (A-Z)
   - `hand`: mÃ£o detectada (Left/Right)
   - 63 features: 21 pontos Ã— 3 coordenadas (x, y, z)

**Dados coletados:**
```csv
label,hand,0_x,0_y,0_z,1_x,1_y,1_z,...,20_x,20_y,20_z
A,Right,0.5,0.6,0.1,0.52,0.58,0.09,...
```

---

### 2ï¸âƒ£ Treinamento do Modelo (`2_train_model.py`)

**Objetivo:** Treinar um modelo de classificaÃ§Ã£o para reconhecer as letras.

**Passos detalhados:**

#### ğŸ“Š **PrÃ©-processamento: NormalizaÃ§Ã£o Relativa ao Pulso**

Para tornar o modelo **invariante Ã  posiÃ§Ã£o** da mÃ£o na tela:

1. Para cada amostra, pegamos as coordenadas do **ponto 0 (pulso)**:
   ```python
   wrist_coords = landmarks[0]  # [x_pulso, y_pulso, z_pulso]
   ```

2. **SubtraÃ­mos** as coordenadas do pulso de **todos os 21 pontos**:
   ```python
   relative_landmarks = landmarks - wrist_coords
   ```

3. Isso cria um sistema de coordenadas **relativo ao pulso**, fazendo com que:
   - O gesto "A" seja o mesmo independentemente de onde a mÃ£o estÃ¡ na tela
   - Reduz variaÃ§Ã£o desnecessÃ¡ria nos dados
   - Melhora significativamente a acurÃ¡cia do modelo

#### ğŸ¤– **Modelo: Random Forest Classifier**

ConfiguraÃ§Ã£o otimizada:
```python
RandomForestClassifier(
    n_estimators=150,      # 150 Ã¡rvores de decisÃ£o
    max_depth=20,          # Profundidade mÃ¡xima de 20
    min_samples_leaf=5,    # MÃ­nimo 5 amostras por folha
    random_state=42        # Reprodutibilidade
)
```

**Por que Random Forest?**
- âœ… Alta precisÃ£o em classificaÃ§Ã£o multiclasse
- âœ… Resistente a overfitting
- âœ… RÃ¡pido na inferÃªncia (importante para tempo real)
- âœ… NÃ£o requer normalizaÃ§Ã£o adicional
- âœ… Fornece probabilidades de classe (`predict_proba`)

#### ğŸ“ˆ **DivisÃ£o dos Dados**

- **80% Treino** / **20% Teste**
- Usa `stratify=y` para manter proporÃ§Ã£o de classes balanceada
- Avalia com `accuracy_score`

#### ğŸ’¾ **SaÃ­da**

- Modelo salvo em: `models/libras_model.pkl`
- PrecisÃ£o tÃ­pica: **>90%** (depende da qualidade dos dados coletados)

---

### 3ï¸âƒ£ Teste do Modelo (`3_test_model.py`)

**Objetivo:** Avaliar o desempenho do modelo treinado.

**MÃ©tricas calculadas:**

1. **Classification Report:**
   - Precision (precisÃ£o por classe)
   - Recall (revocaÃ§Ã£o por classe)
   - F1-score (mÃ©dia harmÃ´nica)
   - Support (quantidade de amostras)

2. **Confusion Matrix:**
   - Matriz visual mostrando acertos e erros
   - Salva como imagem em `models/confusion_matrix.png`

3. **Accuracy Score:**
   - AcurÃ¡cia geral do modelo

---

### 4ï¸âƒ£ AplicaÃ§Ã£o em Tempo Real (`4_real_time_app.py`)

**Objetivo:** Reconhecer gestos ao vivo via webcam.

#### ğŸ”„ **Loop Principal**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Captura frame da webcam (OpenCV)                     â”‚
â”‚  2. Converte BGR â†’ RGB                                   â”‚
â”‚  3. Detecta mÃ£o com MediaPipe                            â”‚
â”‚  4. Extrai 21 landmarks (x, y, z)                        â”‚
â”‚  5. Normaliza: subtrai coordenadas do pulso              â”‚
â”‚  6. Passa pelo modelo Random Forest                      â”‚
â”‚  7. Recebe probabilidades de cada classe                 â”‚
â”‚  8. Sistema de suavizaÃ§Ã£o (buffer)                       â”‚
â”‚  9. Exibe resultado na tela                              â”‚
â”‚  10. Repete (loop)                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ğŸ¯ **Sistema de SuavizaÃ§Ã£o Inteligente**

**Problema:** Sem suavizaÃ§Ã£o, a previsÃ£o oscila rapidamente entre letras (instabilidade).

**SoluÃ§Ã£o:** Sistema de buffer com votaÃ§Ã£o por maioria

```python
PREDICTION_BUFFER_SIZE = 10      # Armazena Ãºltimas 10 previsÃµes
CONFIDENCE_THRESHOLD = 0.8       # SÃ³ aceita previsÃµes com 80%+ confianÃ§a
```

**Como funciona:**

1. Modelo faz previsÃ£o e retorna probabilidades:
   ```python
   prediction_probs = model.predict_proba([landmarks])
   confidence = np.max(prediction_probs)  # Maior probabilidade
   predicted_letter = model.classes_[np.argmax(prediction_probs)]
   ```

2. **Filtro de confianÃ§a:** SÃ³ adiciona ao buffer se `confidence >= 0.8`

3. **Buffer de previsÃµes:** MantÃ©m as Ãºltimas 10 previsÃµes aceitas
   ```python
   prediction_buffer = deque(maxlen=10)
   prediction_buffer.append(predicted_letter)
   ```

4. **VotaÃ§Ã£o por maioria:** A letra estÃ¡vel Ã© a que mais aparece no buffer
   ```python
   most_common = max(set(prediction_buffer), key=prediction_buffer.count)
   if prediction_buffer.count(most_common) > 7:  # 70% do buffer
       stable_prediction = most_common
   ```

**Resultado:**
- âœ… Elimina oscilaÃ§Ãµes rÃ¡pidas
- âœ… SÃ³ muda a letra exibida quando hÃ¡ consistÃªncia
- âœ… Melhor experiÃªncia do usuÃ¡rio

#### ğŸ–¥ï¸ **Interface Visual**

- **Linha superior:** PrevisÃ£o instantÃ¢nea + confianÃ§a (atualiza rÃ¡pido)
  - `Pred: A (0.92)` - mostra a letra e confianÃ§a atual

- **Linha inferior (destaque):** Letra estÃ¡vel (muda apenas com consistÃªncia)
  - `Letra Estavel: A` - resultado suavizado

- **Desenho da mÃ£o:** 21 pontos + conexÃµes desenhados sobre a imagem

---

## ğŸ› ï¸ Tecnologias Utilizadas

| Componente              | Tecnologia                       | VersÃ£o/Detalhes                    |
|-------------------------|----------------------------------|------------------------------------|
| **DetecÃ§Ã£o da mÃ£o**     | MediaPipe Hands                  | 21 landmarks em 3D                 |
| **Captura de vÃ­deo**    | OpenCV (cv2)                     | Webcam + processamento de imagem   |
| **Processamento**       | NumPy / Pandas                   | ManipulaÃ§Ã£o de arrays e DataFrames |
| **Modelo de IA**        | Scikit-learn (Random Forest)     | 150 estimadores, depth=20          |
| **PersistÃªncia**        | Joblib                           | SerializaÃ§Ã£o do modelo             |
| **VisualizaÃ§Ã£o**        | Matplotlib / Seaborn             | GrÃ¡ficos e matriz de confusÃ£o      |
| **Linguagem**           | Python                           | 3.8+                               |

---

## ğŸ“ Estrutura de Pastas

```
LibraVision/
â”‚
â”œâ”€â”€ data/                          # ğŸ“Š Dataset coletado
â”‚   â””â”€â”€ libras_data.csv            # Dados dos gestos (gerado no passo 1)
â”‚
â”œâ”€â”€ models/                        # ğŸ¤– Modelos treinados
â”‚   â”œâ”€â”€ libras_model.pkl           # Modelo Random Forest (gerado no passo 2)
â”‚   â””â”€â”€ confusion_matrix.png       # Matriz de confusÃ£o (gerado no passo 3)
â”‚
â”œâ”€â”€ scripts/                       # ğŸ Scripts Python
â”‚   â”œâ”€â”€ 1_collect_data.py          # Coleta de dados via webcam
â”‚   â”œâ”€â”€ 2_train_model.py           # Treinamento do modelo
â”‚   â”œâ”€â”€ 3_test_model.py            # AvaliaÃ§Ã£o do modelo
â”‚   â””â”€â”€ 4_real_time_app.py         # AplicaÃ§Ã£o em tempo real
â”‚
â”œâ”€â”€ requirements.txt               # ğŸ“¦ DependÃªncias do projeto
â”œâ”€â”€ .gitignore                     # ğŸš« Arquivos ignorados pelo Git
â””â”€â”€ README.md                      # ğŸ“– Esta documentaÃ§Ã£o
```

**Nota:** As pastas `data/` e `models/` sÃ£o criadas automaticamente ao executar os scripts.

---

## ğŸš€ Como Executar o Projeto

### 1. PrÃ©-Requisitos

- **Python 3.8 ou superior**
- **Webcam** funcional
- **Sistema operacional:** Windows, macOS ou Linux

---

### 2. InstalaÃ§Ã£o

#### **a. Clone o repositÃ³rio**

```bash
git clone https://github.com/jonassoaress/LibraVision.git
cd LibraVision
```

#### **b. (Recomendado) Crie e ative um ambiente virtual**

**Windows:**
```bash
python -m venv venv
.\venv\Scripts\activate
```

**macOS/Linux:**
```bash
python3 -m venv venv
source venv/bin/activate
```

#### **c. Instale as dependÃªncias**

```bash
pip install -r requirements.txt
```

---

### 3. Fluxo de ExecuÃ§Ã£o

**Importante:** Execute os scripts na ordem numÃ©rica!

---

#### **Etapa 1: Coletar Dados** ğŸ“¸

```bash
python scripts/1_collect_data.py
```

**O que fazer:**
- Siga as instruÃ§Ãµes no terminal
- Para cada letra (A-Z), faÃ§a o gesto correspondente em Libras
- Mantenha a mÃ£o estÃ¡vel enquanto os dados sÃ£o coletados
- O programa captura mÃºltiplas amostras de cada letra
- Os dados sÃ£o salvos em `data/libras_data.csv`

**Dicas:**
- âœ… Use boa iluminaÃ§Ã£o
- âœ… Mantenha o fundo limpo (sem outras mÃ£os ou objetos)
- âœ… Varie levemente a posiÃ§Ã£o/Ã¢ngulo da mÃ£o entre amostras
- âœ… Colete pelo menos 100 amostras por letra para melhor precisÃ£o

---

#### **Etapa 2: Treinar o Modelo** ğŸ¤–

```bash
python scripts/2_train_model.py
```

**O que acontece:**
- Carrega os dados de `data/libras_data.csv`
- Aplica normalizaÃ§Ã£o (coordenadas relativas ao pulso)
- Divide em treino (80%) e teste (20%)
- Treina o Random Forest com 150 Ã¡rvores
- Exibe a **precisÃ£o do modelo** no terminal
- Salva o modelo treinado em `models/libras_model.pkl`

**SaÃ­da esperada:**
```
Carregando o dataset...
Realizando engenharia de features (normalizaÃ§Ã£o)...
Dividindo os dados em treino e teste...
Treinando o modelo Random Forest...
Avaliando a precisÃ£o do modelo...
PrecisÃ£o do modelo: 94.32%
Salvando o modelo em models/libras_model.pkl...
Treinamento concluÃ­do e modelo salvo com sucesso.
```

---

#### **Etapa 3 (Opcional): Avaliar o Modelo** ğŸ“Š

```bash
python scripts/3_test_model.py
```

**O que acontece:**
- Carrega o modelo treinado
- Avalia no conjunto de teste
- Exibe **Classification Report** no terminal
- Gera e salva **Confusion Matrix** em `models/confusion_matrix.png`

**Exemplo de saÃ­da:**
```
              precision    recall  f1-score   support

           A       0.96      0.94      0.95        50
           B       0.92      0.95      0.93        48
           C       0.94      0.91      0.92        52
         ...
    accuracy                           0.94      1200
   macro avg       0.94      0.94      0.94      1200
weighted avg       0.94      0.94      0.94      1200
```

---

#### **Etapa 4: Executar a AplicaÃ§Ã£o** ğŸ¥

```bash
python scripts/4_real_time_app.py
```

**O que acontece:**
- Abre a webcam
- Detecta sua mÃ£o em tempo real
- Desenha os 21 pontos sobre a mÃ£o
- Exibe:
  - **Pred:** PrevisÃ£o instantÃ¢nea com confianÃ§a
  - **Letra Estavel:** Resultado suavizado (muda apenas com consistÃªncia)

**Controles:**
- Pressione **'q'** para sair

**Dicas para melhor reconhecimento:**
- âœ… Posicione a mÃ£o no centro da tela
- âœ… Mantenha boa iluminaÃ§Ã£o
- âœ… FaÃ§a o gesto de forma clara e estÃ¡vel
- âœ… Aguarde alguns frames para a "Letra Estavel" aparecer

---

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Ajustar ParÃ¢metros do Modelo (`2_train_model.py`)

```python
model = RandomForestClassifier(
    n_estimators=150,      # â†‘ Aumentar = mais precisÃ£o, mais lento
    max_depth=20,          # â†‘ Aumentar = mais complexo, risco de overfit
    min_samples_leaf=5,    # â†“ Diminuir = mais complexo
    random_state=42
)
```

### Ajustar SuavizaÃ§Ã£o (`4_real_time_app.py`)

```python
PREDICTION_BUFFER_SIZE = 10      # â†‘ Aumentar = mais suave, mais lento
CONFIDENCE_THRESHOLD = 0.8       # â†‘ Aumentar = mais restritivo
```

### Ajustar Sensibilidade do MediaPipe (`4_real_time_app.py`)

```python
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=1,
    min_detection_confidence=0.7,   # â†‘ Aumentar = menos falsos positivos
    min_tracking_confidence=0.7     # â†‘ Aumentar = rastreamento mais estÃ¡vel
)
```

---

## â“ Troubleshooting

### **Erro: "Webcam nÃ£o encontrada"**
- Verifique se hÃ¡ outra aplicaÃ§Ã£o usando a webcam
- Tente trocar `cv2.VideoCapture(0)` para `cv2.VideoCapture(1)` em `4_real_time_app.py`

### **Erro: "libras_model.pkl nÃ£o encontrado"**
- Execute o passo 2 (`2_train_model.py`) antes do passo 4

### **Erro: "libras_data.csv nÃ£o encontrado"**
- Execute o passo 1 (`1_collect_data.py`) antes do passo 2

### **PrecisÃ£o muito baixa (<80%)**
- Colete mais dados (>100 amostras por letra)
- Verifique se os gestos foram feitos corretamente
- Use melhor iluminaÃ§Ã£o durante a coleta
- Varie a posiÃ§Ã£o/Ã¢ngulo da mÃ£o nas amostras

### **Letra oscila muito na tela**
- Aumente `PREDICTION_BUFFER_SIZE` (ex: 15)
- Aumente `CONFIDENCE_THRESHOLD` (ex: 0.85)

### **MÃ£o nÃ£o Ã© detectada**
- Diminua `min_detection_confidence` para 0.5
- Melhore a iluminaÃ§Ã£o
- Certifique-se que a mÃ£o estÃ¡ visÃ­vel e aberta

---

## ğŸ“Š Resultados Esperados

Com **100+ amostras por letra** e **boa qualidade de dados**:

- âœ… **AcurÃ¡cia do modelo:** 90-95%
- âœ… **FPS da aplicaÃ§Ã£o:** 20-30 frames/segundo
- âœ… **LatÃªncia de reconhecimento:** <1 segundo (com suavizaÃ§Ã£o)
- âœ… **Taxa de falsos positivos:** <5%

---

## ğŸ¯ PrÃ³ximos Passos / Melhorias Futuras

- [ ] Adicionar suporte para **palavras e frases** (nÃ£o apenas letras)
- [ ] Implementar **deep learning** (CNN/LSTM) para maior precisÃ£o
- [ ] Criar **interface grÃ¡fica** (Tkinter/PyQt)
- [ ] Desenvolver **aplicativo mobile** (Android/iOS)
- [ ] Adicionar **reconhecimento de gestos dinÃ¢micos** (movimentos)
- [ ] Implementar **dataset pÃºblico** para treino
- [ ] Adicionar **suporte multilÃ­ngue** (ASL, BSL, etc.)
- [ ] Otimizar para **edge devices** (Raspberry Pi, Jetson Nano)

---

## ğŸ‘¥ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se livre para:
- Reportar bugs
- Sugerir novas funcionalidades
- Melhorar a documentaÃ§Ã£o
- Enviar pull requests

---

## ğŸ“„ LicenÃ§a

Este projeto Ã© para **fins acadÃªmicos e educacionais**.

---

## ğŸ“§ Contato

- **RepositÃ³rio:** [github.com/jonassoaress/LibraVision](https://github.com/jonassoaress/LibraVision)
- **Desenvolvedor:** Jonas Soares

---

## ğŸ™ Agradecimentos

- **MediaPipe** (Google) - Framework de detecÃ§Ã£o de mÃ£os
- **OpenCV** - Biblioteca de VisÃ£o Computacional
- **Scikit-learn** - Framework de Machine Learning
- Comunidade surda brasileira pela importÃ¢ncia da Libras

---

**â­ Se este projeto foi Ãºtil, considere dar uma estrela no GitHub!**
